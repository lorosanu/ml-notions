<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="activation-functions-in-neural-networks"><span class="header-section-number">1</span> Activation functions in neural networks</h1>
<h2 id="sigmoid-fx-frac11mathrme-x"><span class="header-section-number">1.1</span> Sigmoid <span class="math">\(f(x) = \frac{1}{1+\mathrm{e}^{-x}}\)</span></h2>
<p>Description</p>
<ul>
<li>squashes numbers to range [0, 1]<br /> high values near 1, high negative values near 0</li>
<li>has a nice interpretation of saturating the &quot;firing rate&quot; of a neuron</li>
</ul>
<p>Problems</p>
<ul>
<li>saturated neurons &quot;kill&quot; the gradient<br /> high positive and high negative values generate ~0 gradients (flat slope)</li>
<li>sigmoid outputs are not zero-centered (inneficient gradient updates)</li>
<li>the exponential function is computationally expensive</li>
</ul>
<h2 id="tanh-fx-tanhx"><span class="header-section-number">1.2</span> Tanh <span class="math">\(f(x) = tanh(x)\)</span></h2>
<p>Description</p>
<ul>
<li>squashes numbers to range [-1, 1]<br /> high values near 1, high negative values near -1</li>
<li>outputs are zero-centered</li>
</ul>
<p>Problems</p>
<ul>
<li>saturated neurons &quot;kill&quot; the gradient</li>
</ul>
<h2 id="relu-rectified-linear-unit-fx-max0-x"><span class="header-section-number">1.3</span> ReLU (REctified Linear Unit) <span class="math">\(f(x) = max(0, x)\)</span></h2>
<p>Description</p>
<ul>
<li>does not saturate in the positive region</li>
<li>very computationally efficient</li>
<li>converges much faster than sigmoid/tanh in practice</li>
</ul>
<p>Problems</p>
<ul>
<li>not zero-centered output</li>
<li>saturated neurons in the negative region</li>
<li>dead ReLUs will never activate and therefore will never update</li>
</ul>
<h2 id="leaky-relu-fx-max0.1-x-x"><span class="header-section-number">1.4</span> Leaky ReLU <span class="math">\(f(x) = max(0.1 x, x)\)</span></h2>
<p>Description</p>
<ul>
<li>does not saturate</li>
<li>computationally efficien</li>
<li>converges faster than sigmoid/tanh in practice</li>
<li>will not die</li>
</ul>
</body>
</html>
