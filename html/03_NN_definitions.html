<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="definitions"><span class="header-section-number">1</span> Definitions</h1>
<h2 id="gradient-descent"><span class="header-section-number">1.1</span> Gradient descent</h2>
<ul>
<li>gradient descent is an iterative optimization algorithm used to minimize a function<br /> by iteratively moving in the direction of <em>steepest descent</em> as defined by the <em>negative of the gradient</em></li>
<li>use gradient descent to update the parameters (weights <span class="math">\(w\)</span>, <span class="math">\(b\)</span>) of the model</li>
<li>find the optimal weights that reduce the prediction error (minimize loss)</li>
</ul>
<p>Gradient descent algorithm</p>
<ul>
<li>s1: initialize the weights with random values and calculate error</li>
<li>s2: calculate the gradient (the change in error when the weights are changed by a very small amount);<br /></li>
<li>s3: adjust weights with their gradients; helps move the weights in the direction in which the error is minimized</li>
<li>s4: use the new weights for prediction and to calculate the new error</li>
<li>s5: repeat steps 2 to 4 untill no significant error reduction</li>
</ul>
<h2 id="gradient-ascent"><span class="header-section-number">1.2</span> Gradient ascent</h2>
<p>The optimization alorithm that takes steps proportional to the <em>positive of the gradient</em>, thus approaching a local maximum of that function.</p>
<h2 id="overfitting"><span class="header-section-number">1.3</span> Overfitting</h2>
<ul>
<li>when the model is trying too hard to capture the noise in the training dataset</li>
<li>it models the training data too well</li>
<li>it doesn't generalize well to new data</li>
<li><strong>solutions</strong> for overfitting (<em>high variance</em>)
<ul>
<li>get more data (data augmentation) (e.g. rotations, flipping, zooming, distortions in images)</li>
<li>try regularization</li>
<li>try a different network architecture</li>
<li>try early stopping</li>
</ul></li>
</ul>
<h2 id="underfitting"><span class="header-section-number">1.4</span> Underfitting</h2>
<ul>
<li>the model fails to correctly model the training data</li>
<li>it also doesn't generalize to new data</li>
<li><strong>solutions</strong> for underfitting (<em>high bias</em>)
<ul>
<li>use a more complex model or a deeper model
<ul>
<li>try bigger network</li>
<li>try a different network architecture</li>
</ul></li>
<li>train it longer</li>
<li>try some optimization algorithms</li>
</ul></li>
</ul>
<h2 id="vanishing-and-exploding-gradients"><span class="header-section-number">1.5</span> Vanishing and exploding gradients</h2>
<ul>
<li>when training very deep neural networks the derivatives can end up either very very big or very very small, which makes training difficult</li>
<li>the derivatives might increase exponentially or decrease exponentially as a function of <em>L</em> (number of layers), depending on the wights initial values</li>
<li>make very carefull choices when initializing the weights in order to significantly reduce this problem</li>
</ul>
</body>
</html>
