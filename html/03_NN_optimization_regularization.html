<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="regularization-in-neural-networks"><span class="header-section-number">1</span> Regularization in neural networks</h1>
<h2 id="definition"><span class="header-section-number">1.1</span> Definition</h2>
<p>A process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting.<br />It penalizes the loss function by adding a multiple of an L1 (Lasso) or an L2 (Ridge) norm of your weights vector <span class="math">\(w\)</span>.</p>
<h2 id="why"><span class="header-section-number">1.2</span> Why</h2>
<p>Solve the overfitting problem.</p>
<h2 id="formulation"><span class="header-section-number">1.3</span> Formulation</h2>
<p>Cost function over <em>m</em> training examples</p>
<p><span class="math">\(\frac{1}{m} L(\hat{y}^{(i)}, y^{(i)}) \ + \ \lambda \ \ast \ R(w)\)</span></p>
<h2 id="hyperparameters"><span class="header-section-number">1.4</span> Hyperparameters</h2>
<ul>
<li><span class="math">\(\lambda\)</span>: the regularization parameter</li>
</ul>
<h1 id="variations"><span class="header-section-number">2</span> Variations</h1>
<h2 id="l1-regulatization"><span class="header-section-number">2.1</span> L1 regulatization</h2>
<p>Adds the absolute values of the model's coefficients as the penalty term.</p>
<p><span class="math">\(R(w) = \frac{1}{m} \sum_{l=1}^{L} |w^{[l]}|\)</span></p>
<h2 id="l2-regulatization"><span class="header-section-number">2.2</span> L2 regulatization</h2>
<p>Adds the squared magnitude of the model's coefficients as the penalty term.</p>
<p><span class="math">\(R(w) = \frac{1}{2m} \sum_{l=1}^{L} ||w^{[l]}||^2 = \frac{1}{2m} \sum_{l=1}^{L} \sum_{i=i}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^2\)</span></p>
<p>New formula for weight update</p>
<p><span class="math">\(W^{[l]} = W^{[l]} \ - \ \alpha \ \ast \ dW^{[l]} = W^{[l]} - \alpha \ \ast \ (amount\ from\ backprob \ + \ \frac{\lambda}{m} W^{[l]}) \)</span></p>
<h2 id="elastic-net-l1-l2"><span class="header-section-number">2.3</span> Elastic net (L1 + L2)</h2>
<p>Adds both L1 and L2 penalities.</p>
<p><span class="math">\(R(w) =  \sum_{l=1}^{L}\sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} \left( \beta \ \ast \ (w_{i,j}^{[l]})^2 \ + \ |w_{i,j}^{[l]}| \right)\)</span></p>
</body>
</html>
