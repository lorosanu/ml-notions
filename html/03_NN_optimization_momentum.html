<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="gradient-descent-with-momentum"><span class="header-section-number">1</span> Gradient descent with momentum</h1>
<p><strong>Definition</strong></p>
<ul>
<li>gradient descent with momentum is an optimization algorithm which relies on<br /> computing the <strong>exponentially weighted (moving) averages</strong> of gradients<br /> and using that gradient to update the weights</li>
<li>build up &quot;velocity&quot; as a running mean of gradients<br /></li>
<li>step in the direction of the velocity over time</li>
</ul>
<p><strong>Why</strong></p>
<ul>
<li>move faster towards the minimum loss goal.</li>
</ul>
<p><strong>Formulation</strong></p>
<ul>
<li><p>The computation of the exponentially weighted averages</p>
<ul>
<li><span class="math">\(V_{0} = 0\)</span></li>
<li>...</li>
<li><span class="math">\(V_{t} \ = \ \beta\ \ast \ V_{t-1} \ + \ (1 - \beta)\ \ast \ \theta_t\)</span></li>
</ul></li>
<li><p><span class="math">\(V_t\)</span> is approximately averaging over <span class="math">\(\frac{1}{1 - \beta}\)</span> previous data points</p>
<ul>
<li>for <span class="math">\(\beta=0.5\)</span>, <span class="math">\(V_t\)</span> is averaging over the last 2 values</li>
<li>for <span class="math">\(\beta=0.9\)</span>, <span class="math">\(V_t\)</span> is averaging over the last 10 values</li>
<li>for <span class="math">\(\beta=0.98\)</span>, <span class="math">\(V_t\)</span> is averaging over the last 50 values</li>
</ul></li>
</ul>
<p><strong>Bias correction</strong></p>
<ul>
<li>problem: fix the initial low estimates due to initializing <span class="math">\(V_0\)</span> to zero</li>
<li>solution: replace <span class="math">\(V_t\)</span> with <span class="math">\(\frac{V_t}{1 - \beta^t}\)</span> (take into account the current time step)</li>
<li>not often used in practice; people usually prefer waiting the exponentially weighted averaged to simply finish warming up</li>
</ul>
<h1 id="variations"><span class="header-section-number">2</span> Variations</h1>
<h2 id="mini-batch-gd-with-momentum-smooth-out-the-steps-of-gradient-descent"><span class="header-section-number">2.1</span> Mini-batch GD with momentum: smooth out the steps of gradient descent</h2>
<p><strong>Implementation</strong></p>
<ul>
<li><p>initialize</p>
<ul>
<li><span class="math">\(V_{dw} = 0\)</span></li>
<li><span class="math">\(V_{db} = 0\)</span></li>
</ul></li>
<li><p>compute <span class="math">\(dw\)</span> and <span class="math">\(db\)</span> for curent minibatch</p></li>
<li><p>compute the exponentially weighted averages</p>
<ul>
<li><span class="math">\(V_{dw} = \beta \ \ast \ V_{dw} + (1 - \beta)\ \ast \ dw\)</span></li>
<li><span class="math">\(V_{db} = \beta \ \ast \ V_{db} + (1 - \beta)\ \ast \ db\)</span></li>
</ul></li>
<li><p>update the weights</p>
<p><span class="math">\(w = w - \alpha \ \ast \ V_{dw}\)</span></p>
<p><span class="math">\(b = b - \alpha \ \ast \ V_{db}\)</span></p></li>
</ul>
<p><strong>Hyperparameters</strong></p>
<ul>
<li><p><span class="math">\(\alpha\)</span>: needs to be tuned</p></li>
<li><p><span class="math">\(\beta = 0.9\)</span> (average over ~ 10 gradients)</p></li>
</ul>
<h2 id="rmsprop-root-mean-squared-prop-can-also-speed-up-gradient-descent"><span class="header-section-number">2.2</span> RMSprop (Root Mean Squared prop): can also speed up gradient descent</h2>
<p><strong>Implementation</strong></p>
<ul>
<li><p>initialize</p>
<ul>
<li><span class="math">\(S_{dw} = 0\)</span></li>
<li><span class="math">\(S_{db} = 0\)</span></li>
</ul></li>
<li><p>compute <span class="math">\(dw\)</span> and <span class="math">\(db\)</span> for curent minibatch</p></li>
<li><p>compute the exponentially weighted averages</p>
<ul>
<li><span class="math">\(S_{dw} = \beta \ \ast \ V_{dw} + (1 - \beta) \ \ast \ dw^2\)</span> (element-wise squaring operation)<br /></li>
<li><span class="math">\(S_{db} = \beta \ \ast \ V_{db} + (1 - \beta) \ \ast \ db^2\)</span> (element-wise squaring operation)</li>
</ul></li>
<li><p>update the weights</p>
<ul>
<li><span class="math">\(w = w - \alpha \ \ast \ \frac{dw}{\sqrt{S_{dw} + \varepsilon}}\)</span></li>
<li><span class="math">\(b = b - \alpha \ \ast \ \frac{db}{\sqrt{S_{db} + \varepsilon}}\)</span></li>
</ul></li>
</ul>
<p><strong>Hyperparameters</strong></p>
<ul>
<li><p><span class="math">\(\alpha\)</span>: needs to be tuned</p></li>
<li><p><span class="math">\(\beta = 0.999\)</span></p></li>
<li><p><span class="math">\(\varepsilon = 1\mathrm{e}-8\)</span> (just to avoid zero-division errors)</p></li>
</ul>
<h2 id="adam-adaptive-moment-estimation-combines-momentum-with-rsmprop"><span class="header-section-number">2.3</span> ADAM (ADAptive Moment estimation): combines momentum with RSMprop</h2>
<p><strong>Implementation</strong></p>
<ul>
<li><p>initialize</p>
<ul>
<li><span class="math">\(V_{dw} = 0\)</span></li>
<li><span class="math">\(V_{db} = 0\)</span></li>
<li><span class="math">\(S_{dw} = 0\)</span></li>
<li><span class="math">\(S_{db} = 0\)</span></li>
</ul></li>
<li><p>compute <span class="math">\(dw\)</span> and <span class="math">\(db\)</span> for curent minibatch</p></li>
<li><p>compute the exponentially weighted averages</p>
<ul>
<li><span class="math">\(V_{dw} = \beta_1 \ \ast \ V_{dw} + (1 - \beta_1) \ \ast \ dw\)</span><br /></li>
<li><span class="math">\(V_{db} = \beta_1 \ \ast \ V_{db} + (1 - \beta_1) \ \ast \ db\)</span><br /></li>
<li><span class="math">\(S_{dw} = \beta_2 \ \ast \ V_{dw} + (1 - \beta_2) \ \ast \ dw^2\)</span> (element-wise squaring operation)</li>
<li><span class="math">\(S_{db} = \beta_2 \ \ast \ V_{db} + (1 - \beta_2) \ \ast \ db^2\)</span> (element-wise squaring operation)</li>
</ul></li>
<li><p>apply bias correction</p>
<ul>
<li><span class="math">\(V_{dw}^{corrected} = \frac{V_{dw}}{1 - \beta_1^{t}}\)</span><br /></li>
<li><span class="math">\(V_{db}^{corrected} = \frac{V_{db}}{1 - \beta_1^{t}}\)</span><br /></li>
<li><span class="math">\(S_{dw}^{corrected} = \frac{S_{dw}}{1 - \beta_2^{t}}\)</span><br /></li>
<li><span class="math">\(S_{db}^{corrected} = \frac{S_{db}}{1 - \beta_2^{t}}\)</span></li>
</ul></li>
<li><p>update the weights</p>
<ul>
<li><p><span class="math">\(w = w - \alpha \ \ast \ \frac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected} + \varepsilon}}\)</span></p></li>
<li><p><span class="math">\(b = b - \alpha \ \ast \ \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected} + \varepsilon}}\)</span></p></li>
</ul></li>
</ul>
<p><strong>Hyperparameters</strong></p>
<ul>
<li><p><span class="math">\(\alpha\)</span>: needs to be tuned</p></li>
<li><p><span class="math">\(\beta_1 = 0.9\)</span></p></li>
<li><p><span class="math">\(\beta_2 = 0.999\)</span></p></li>
<li><p><span class="math">\(\varepsilon = 1\mathrm{e}-8\)</span> (just to avoid zero-division errors)</p></li>
</ul>
</body>
</html>
