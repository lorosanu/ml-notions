<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="linear-regression-for-regression"><span class="header-section-number">1</span> Linear regression (for regression)</h1>
<p>Model a scalar target with one or more quantitative features.<br />Although regression computes a linear combination, features can be transformed by nonlinear functions if relationships are known or can be guessed.</p>
<div class="figure">
<img src="http://pgfplots.net/media/tikz/examples/PNG/regression-line.png" alt="Example of prediction using a linear regression model" /><p class="caption">Example of prediction using a linear regression model</p>
</div>
<h2 id="univariate-linear-regression"><span class="header-section-number">1.1</span> Univariate linear regression</h2>
<ul>
<li><p>Description</p>
Simple linear regression is a statistical method that studies the relationship between two variables:
<ul>
<li><span class="math">\(x\)</span>: the predictor, explanatory, independent variable,</li>
<li><span class="math">\(y\)</span>: the response, outcome, dependent variable.</li>
</ul></li>
<li><p>Model's hypothesis</p>
<p><span class="math">\(h(x) = \theta_0 + \theta_1 \ast x = \hat{y}\)</span></p></li>
<li>Model's parameters (# <span class="math">\(2\)</span>)
<ul>
<li><span class="math">\(\theta_0\)</span></li>
<li><span class="math">\(\theta_1\)</span></li>
</ul></li>
<li><p>Cost function (in this case, the squared error function)</p>
<p><span class="math">\(J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right)^2\)</span></p></li>
<li><p>Goal</p>
<p><span class="math">\(minimize_{\theta_0, \theta_1} J(\theta_0, \theta_1)\)</span></p></li>
<li><p>Algorithm</p>
<ul>
<li><p>gradient descent</p>
<ul>
<li>start with some initial values for <span class="math">\(\theta_0\)</span> and <span class="math">\(\theta_1\)</span> (usually zero)</li>
<li><p>keep changing <span class="math">\(\theta_0\)</span> and <span class="math">\(\theta_1\)</span> to reduce <span class="math">\(J(\theta_0, \theta_1)\)</span></p>
<ul>
<li><p><span class="math">\(\theta_0 = \theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) = \theta_0 - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \right)\)</span></p></li>
<li><p><span class="math">\(\theta_1 = \theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) = \theta_1 - \alpha \left(  \frac{1}{m} \sum_{i=1}^{m} \left(\hat{y}^{i} - y^{(i)} \right) \cdot x^{(i)}\right)\)</span></p></li>
</ul></li>
</ul></li>
<li><p>directly use the following <strong>formulas</strong></p>
<ul>
<li><p><span class="math">\(\theta_1 = \frac{\sum_{i=1}^{m} (x^{(i)} - \bar{x}) (y^{(i)} - \bar{y})}{\sum_{i=1}^{m} (x^{(i)} - \bar{x})^2}\)</span></p></li>
<li><p><span class="math">\(\theta_0 = \bar{y} - \theta_1 \bar{x}\)</span></p></li>
</ul></li>
</ul></li>
<li><p>Notations</p>
<ul>
<li><span class="math">\(x\)</span>: the independent variable</li>
<li><span class="math">\(y\)</span>: the dependent variable</li>
<li><span class="math">\(m\)</span>: the number of training examples</li>
<li><span class="math">\(x^{(i)}\)</span>: the input value of the <span class="math">\(i^{th}\)</span> training example</li>
<li><span class="math">\(y^{(i)}\)</span>: the target value of the <span class="math">\(i^{th}\)</span> training example</li>
<li><span class="math">\(\hat{y}^{(i)}\)</span>: the prediction made on the <span class="math">\(i^{th}\)</span> training example by the current hypothesis function</li>
<li><span class="math">\(\alpha\)</span>: the learning rate; determines how big steps we take when updating the <span class="math">\(\theta\)</span> parameters</li>
</ul></li>
<li><p>Hyperparameters:</p>
<ul>
<li><span class="math">\(\alpha\)</span>: if too small, slow gradient descent; if too large, gradient descent may fail to converge</li>
</ul></li>
<li><p>Problems:</p>
<ul>
<li>gradient descent may converge to a local minimum</li>
</ul></li>
</ul>
<h2 id="multivariate-linear-regression"><span class="header-section-number">1.2</span> Multivariate linear regression</h2>
<ul>
<li><p>Description</p>
Multivariate linear regression is a statistical method that studies the relationship between multiple variables:
<ul>
<li><span class="math">\(n\)</span> variables <span class="math">\(x=\{x_1,x_2,...,x_n\}\)</span>: the predictor, explanatory, independent variables,</li>
<li>one <span class="math">\(y\)</span> variable: the response, outcome, dependent variable.</li>
</ul></li>
<li><p>Model's hypothesis</p>
<p><span class="math">\(h(x) = \theta_0 + \theta_1 \ast x_1  + \theta_2 \ast x_2 + ... + \theta_n \ast x_n = \hat{y}\)</span></p></li>
<li><p>Model's parameters (# <span class="math">\(n+1\)</span>)</p>
<p><span class="math">\(\theta=\{\theta_0, \theta_1, ..., \theta_n\}\)</span></p></li>
<li><p>Cost function (in this case, the squared error function)</p>
<p><span class="math">\(J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2\)</span></p></li>
<li><p>Goal</p>
<p><span class="math">\(minimize_{\theta} J(\theta)\)</span></p></li>
<li><p>Algorithm</p>
<ul>
<li><p>gradient descent</p>
<ul>
<li>start with some initial values for <span class="math">\(\theta_0\)</span>, <span class="math">\(\theta_1\)</span>, ..., <span class="math">\(\theta_n\)</span> (usually <span class="math">\(0\)</span>)</li>
<li><p>keep changing <span class="math">\(\theta\)</span>s to reduce <span class="math">\(J(\theta)\)</span></p>
<ul>
<li><p><span class="math">\(\theta_0 = \theta_0 - \alpha \frac{\partial J}{\partial \theta_0} = \theta_0 - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \right)\)</span></p></li>
<li><p><span class="math">\(\theta_1 = \theta_1 - \alpha \frac{\partial J}{\partial \theta_1} = \theta_1 - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_1^{(i)}\right)\)</span></p></li>
<li><p>...</p></li>
<li><p><span class="math">\(\theta_n = \theta_n - \alpha \frac{\partial J}{\partial \theta_n} = \theta_n - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_n^{(i)}\right)\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Notations</p>
<ul>
<li><span class="math">\(x\)</span>: the independent variables</li>
<li><span class="math">\(y\)</span>: the dependent variable</li>
<li><span class="math">\(m\)</span>: the number of training examples</li>
<li><span class="math">\(n\)</span>: the number of features representing each training example</li>
<li><span class="math">\(x^{(i)}\)</span>: the input values of the <span class="math">\(i^{th}\)</span> training example</li>
<li><span class="math">\(x_j^{(i)}\)</span>: the value of the <span class="math">\(j^{th}\)</span> feature of the <span class="math">\(i^{th}\)</span> training example</li>
<li><span class="math">\(y^{(i)}\)</span>: the target value of the <span class="math">\(i^{th}\)</span> training example</li>
<li><span class="math">\(\hat{y}^{(i)}\)</span>: the prediction made on the <span class="math">\(i^{th}\)</span> training example by the current hypothesis function</li>
<li><span class="math">\(\alpha\)</span>: the learning rate; determines how big steps we take when updating the <span class="math">\(\theta\)</span> parameters</li>
</ul></li>
<li><p>Hyperparameters:</p>
<ul>
<li><span class="math">\(\alpha\)</span></li>
</ul></li>
<li><p>Problems:</p>
<ul>
<li><p>make sure features are on similar scales; gradient descent may be slow otherwise</p>
<ul>
<li><p>rescaling: <span class="math">\(x&#39; = \frac{x - min(x)}{max(x) - min(x)}\)</span></p></li>
<li><p>mean normalization: <span class="math">\(x&#39; = \frac{x - mean(x)}{max(x) - min(x)}\)</span></p></li>
<li><p>standardization: <span class="math">\(x&#39; = \frac{x - mean(x)}{std(x)}\)</span></p></li>
</ul></li>
<li><p>make sure the gradient descent is working correctly</p>
<ul>
<li>plot the value of the cost function <span class="math">\(J\)</span> over the number of iterations (# <span class="math">\(epochs\)</span>)</li>
<li>for a sufficiently small <span class="math">\(\alpha\)</span>, <span class="math">\(J(\theta)\)</span> should decrease on every iteration</li>
<li>if <span class="math">\(\alpha\)</span> is too small, the gradient descent can be slow to converge</li>
<li>if <span class="math">\(\alpha\)</span> is too large, <span class="math">\(J(\theta)\)</span> may not decrease on every iteration; may not converge</li>
<li>try values <span class="math">\(\alpha \in \{..., 0.001, 0.01, 0.1, 1, ...\}\)</span></li>
</ul></li>
<li><p>try adding new features if the model doesn't perform well</p>
<ul>
<li><p>e.g, use a polynomial regression</p>
<p><span class="math">\(\theta_0 + \theta_1 \ast x \ \ \ \Rightarrow \ \ \ \theta_0 + \theta_1 \ast x + \theta_2 \ast x^2 + \theta_3 \ast x^3\)</span></p></li>
</ul></li>
<li><p>use regularization in case of overfitting</p>
<ul>
<li><p>cost function with the regularization term</p>
<p><span class="math">\(J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2\)</span></p></li>
<li><p>parameter update in gradient descent (for <span class="math">\(j \in \{1, 2, ..., n\}\)</span>)</p>
<p><span class="math">\(\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j} = \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_j^{(i)} + \frac{\lambda}{m} \theta_j \right) = \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_j^{(i)}\)</span></p></li>
<li><p>intuition: <em>shrink</em> model parameters in order to <em>smooth out</em> the decision boundary</p></li>
<li><p><span class="math">\(\lambda\)</span> is the regularization parameter and needs to be tuned</p></li>
</ul></li>
</ul></li>
</ul>
<h1 id="logistic-regression-for-classification"><span class="header-section-number">2</span> Logistic regression (for classification)</h1>
<p>Categorize observations based on quantitative features. Predict target class or probabilities of target classes.</p>
<div class="figure">
<img src="https://qph.ec.quoracdn.net/main-qimg-a19746bfc7afdfa6d9d0cbaf3f48af88" alt="Example of a binary classification using a logictic regression model" /><p class="caption">Example of a binary classification using a logictic regression model</p>
</div>
<h2 id="binary-classification"><span class="header-section-number">2.1</span> Binary classification</h2>
<ul>
<li><p>Description</p>
Logistic regression is a statistical method that studies the relationship between multiple variables:
<ul>
<li><span class="math">\(n\)</span> variables <span class="math">\(x=\{x_1,x_2,...,x_n\}\)</span>: the predictor, explanatory, independent variables,</li>
<li>one <span class="math">\(y\)</span> variable: the response, outcome, dependent variable.</li>
</ul>
Logistic regression expands the linear regression model with a <em>logistic function</em> to make it suitable for classification.<br />Its dependent variable is therefore categorical instead of numerical.<br />In case of a binary classification task, its dependent variable takes on one out of two possible values
<ul>
<li><span class="math">\(y \in \{0, 1\}\)</span></li>
<li><span class="math">\(0\)</span> indicates the <em>negative</em> class</li>
<li><span class="math">\(1\)</span> indicates the <em>positive</em> class</li>
</ul></li>
<li><p>Model's hypothesis: output the estimated probability that <span class="math">\(y=1\)</span> on input x</p>
<ul>
<li><p><span class="math">\(z = \theta_0 + \theta_1 \ast x_1  + \theta_2 \ast x_2 + ... + \theta_n \ast x_n\)</span></p></li>
<li><p><span class="math">\(h(x) = P(y=1|x, \theta) = \sigma(z) = \frac{1}{1 + \mathrm{e}^{-z}} = \hat{y}\ \ \ \)</span> (<span class="math">\(0 &lt;= h(x) &lt;= 1\)</span>)</p></li>
</ul></li>
<li><p>Model's parameters (# <span class="math">\(n+1\)</span>)</p>
<p><span class="math">\(\theta=\{\theta_0, \theta_1, ..., \theta_n\}\)</span></p></li>
<li><p>Decision boundary</p>
<ul>
<li><p>linear</p></li>
<li><p>non-linear when adding extra higher-order polynomial terms to the features</p></li>
</ul></li>
<li><p>Cost function</p>
<p><span class="math">\(J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( -y^{(i)}\ log\ \hat{y}^{(i)} - (1 - y^{(i)})\ log\ (1 - \hat{y}^{(i)}) \right)\)</span></p>
in other words
<ul>
<li><span class="math">\(J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( -\ log\ \hat{y}^{(i)} \right)\)</span> when <span class="math">\(y^{(i)}=1\)</span></li>
<li><span class="math">\(J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( -\ log\ (1 - \hat{y}^{(i)}) \right)\)</span> when <span class="math">\(y^{(i)}=0\)</span></li>
</ul></li>
<li><p>Goal</p>
<p><span class="math">\(minimize_{\theta} J(\theta)\)</span></p></li>
<li><p>Algorithm</p>
<ul>
<li><p>gradient descent</p>
<ul>
<li>start with some initial values for <span class="math">\(\theta_0\)</span>, <span class="math">\(\theta_1\)</span>, ..., <span class="math">\(\theta_n\)</span> (usually normal random values)</li>
<li><p>keep changing <span class="math">\(\theta\)</span>s to reduce <span class="math">\(J(\theta)\)</span></p>
<ul>
<li><p><span class="math">\(\theta_0 = \theta_0 - \alpha \frac{\partial J}{\partial \theta_0} = \theta_0 - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \right)\)</span></p></li>
<li><p><span class="math">\(\theta_1 = \theta_1 - \alpha \frac{\partial J}{\partial \theta_1} = \theta_1 - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_1^{(i)}\right)\)</span></p></li>
<li><p>...</p></li>
<li><p><span class="math">\(\theta_n = \theta_n - \alpha \frac{\partial J}{\partial \theta_n} = \theta_n - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_n^{(i)}\right)\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
<li><p>Hyperparameters:</p>
<ul>
<li><span class="math">\(\alpha\)</span></li>
</ul></li>
<li><p>Problems:</p>
<ul>
<li><p>the idea of feature scaling also aplied for logistic regression</p></li>
<li><p>make sure the gradient descent is working correctly</p></li>
<li><p>try adding new features if the model doesn't perform well</p></li>
<li><p>also use regularization in case of overfitting</p>
<ul>
<li><p>cost function with the regularization term</p>
<p><span class="math">\(J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( -y^{(i)}\ log\ \hat{y}^{(i)} - (1 - y^{(i)})\ log\ (1 - \hat{y}^{(i)}) \right) + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2\)</span></p></li>
<li><p>parameter update in gradient descent (for <span class="math">\(j \in \{1, 2, ..., n\}\)</span>)</p>
<p><span class="math">\(\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j} = \theta_j - \alpha \left( \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_j^{(i)} + \frac{\lambda}{m} \theta_j \right) = \theta_j \left( 1 - \alpha \frac{\lambda}{m} \right) - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{i} - y^{(i)} \right) \cdot x_j^{(i)}\)</span></p></li>
<li><p>intuition: <em>shrink</em> model parameters in order to <em>smooth out</em> the decision boundary</p></li>
<li><p><span class="math">\(\lambda\)</span> is the regularization parameter and needs to be tuned</p></li>
</ul></li>
</ul></li>
</ul>
<h2 id="multiclass-classification"><span class="header-section-number">2.2</span> Multiclass classification</h2>
<ul>
<li><p>Description</p>
<ul>
<li>use the one-vs-all (one-vs-rest) approach</li>
<li>turn the problem into <span class="math">\(C\)</span> binary classification problems (generate <span class="math">\(C\)</span> decision boundaries)</li>
<li>formally: train a logistic regression classifier <span class="math">\(h^{(i)}(x)\)</span> for each class <span class="math">\(i\)</span> to predict the probability that <span class="math">\(y=i\)</span></li>
<li>on a new input <span class="math">\(x\)</span>, in order to make a prediction pick the class <span class="math">\(i\)</span> that maximizes <span class="math">\(max_i h^{(i)}(x)\)</span></li>
</ul></li>
</ul>
</body>
</html>
