<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="neural-networks-for-classification-or-regression"><span class="header-section-number">1</span> Neural networks (for classification or regression)</h1>
<p>Used to estimate unknown functions (complex, non-linear hypothesis) that are based on a large number of inputs, through the back-propagation algorithm.<br />Generally more complex and computationally expensive than other methods, but powerful for certain problems.<br />The basis of many deep learning methods.<br />Today is the state of the art technique for many different machine learning problems.</p>
<h2 id="description"><span class="header-section-number">1.1</span> Description</h2>
<p>Neural networks were developed to vaguely simulate the neurons in the brain.</p>
<div class="figure">
<img src="../images/nn-1.png" />
</div>
<p>A neuron is a computational unit that receives a number of inputs through its input wires (<em>dendrites</em>),<br />does some computation and then sends signals through its output wire (<em>axon</em>) to other neurons in the brain.</p>
<p>A neural network is a group of neurons.<br />The inputs are grouped in an <em>input layer</em>.<br />The outputs are grouped in a final <em>output layer</em>.<br />The layers in between are called the <em>hidden layers</em>.<br />Adding more layers helps computing even more complex functions on the input data.</p>
<div class="figure">
<img src="../images/nn-2.png" />
</div>
<h2 id="notation"><span class="header-section-number">1.2</span> Notation</h2>
<ul>
<li><p>training set <span class="math">\(\left\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\right\}\)</span> with <span class="math">\(m\)</span> training samples</p></li>
<li><p>each input variable has <span class="math">\(n\)</span> features: <span class="math">\(x^{(i)}=\{x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}\}\)</span></p></li>
<li><p>output variable <span class="math">\(y^{(i)}\)</span>, represented by either a single value (in case of regression or binary classification) or by an identity vector (in case of multi-class classification)</p></li>
<li><p><span class="math">\(L\)</span>: the total number of layers in the network (comprising the hidden layers and the output layer)</p></li>
<li><p><span class="math">\(n^{[l]}\)</span>: the number of units (neurons) in layer <span class="math">\(l\)</span></p></li>
<li><p><span class="math">\(w^{[l]}\)</span>: weights matrix <span class="math">\(\left[ n^{[l]} \ \times \ n^{[l-1]} \right]\)</span> controlling function mapping from layer <span class="math">\(l-1\)</span> to layer <span class="math">\(l\)</span>; <span class="math">\(w_{ij}^{[l]}\)</span>: weight to unit <span class="math">\(i\)</span> in layer <span class="math">\(l\)</span> from unit <span class="math">\(j\)</span> in layer <span class="math">\(l-1\)</span></p></li>
<li><p><span class="math">\(b^{[l]}\)</span>: bias vector <span class="math">\(\left[ n^{[l]}, 1 \right]\)</span> on layer <span class="math">\(l\)</span>; <span class="math">\(b_{i}^{[l]}\)</span>: bias on unit <span class="math">\(i\)</span> in layer <span class="math">\(l\)</span></p></li>
<li><p>activation values outputed from layer <span class="math">\(l\)</span></p>
<p><span class="math">\(z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}\)</span></p>
<p><span class="math">\(a^{[l]} = g^{[l]}(z^{[l]})\)</span></p></li>
<li><p>in detail for each unit in the layer</p>
<p><span class="math">\(z_j^{[l]} = w_{j0}^{[l]} a_0^{[l-1]} + w_{j1}^{[l]} a_1^{[l-1]} + ... + w_{jn^{[l-1]}}^{[l]} a_{n^{[l-1]}}^{[l-1]} + b_j^{[l]}\)</span></p>
<p><span class="math">\(a_j^{[l]} = g^{[l]}(z_j^{[l]})\)</span></p></li>
<li><p>output <span class="math">\(\hat{y} = a^{[L]} = h(x)\)</span></p></li>
</ul>
<h2 id="neural-networks-for-multi-class-classification"><span class="header-section-number">1.3</span> Neural networks for multi-class classification</h2>
<ul>
<li><p>Model's architecture</p>
<ul>
<li>fully connected network</li>
<li><span class="math">\(L\)</span> layers</li>
<li>input layer with <span class="math">\(m\)</span> units (a training sample <span class="math">\(x\)</span>)</li>
<li>output layer with <span class="math">\(K\)</span> units (<span class="math">\(\hat{y} = h_{\Theta}(x) \in \mathbb{R}^{K}\)</span>)</li>
<li><span class="math">\(L-1\)</span> hidden layers with <span class="math">\(s_{l}\)</span> units (for <span class="math">\(l \in \{1,...,L-1\}\)</span>)</li>
</ul></li>
<li><p>Model's parameters</p>
<p><span class="math">\(w=\left\{ w^{[1]}, w^{[2]}, ..., w^{[L]} \right\}\)</span><br /><span class="math">\(b=\left\{ b^{[1]}, b^{[2]}, ..., b^{[L]} \right\}\)</span></p></li>
<li><p>Cost function (depends on the output activation function)</p>
<p><span class="math">\(J(w, b) = \frac{1}{m} \sum_{i=1}^{m} \mathscr{L}(\hat{y}, y) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left( -y_k^{(i)}\ log\ \hat{y}_k^{(i)} - (1 - y_k^{(i)})\ log\ (1 - \hat{y}_k^{(i)}) \right)\)</span></p></li>
<li><p>Goal</p>
<p><span class="math">\(\underset{w, b}{\operatorname{min}} J(w, b)\)</span></p></li>
<li><p>Algorithm</p>
<ul>
<li><p>start with some initial values for <span class="math">\(w\)</span> and <span class="math">\(b\)</span> (usually random values in <span class="math">\([-\epsilon, \epsilon]\)</span>)</p></li>
<li><p>for each epoch</p>
<ul>
<li><p>set <span class="math">\(a^{[0]} = x\)</span></p></li>
<li><p>perform forward-propagation to compute <span class="math">\(a^{[l]}\)</span> for <span class="math">\(l=\{1, 2, ..., L\}\)</span></p>
<ul>
<li><span class="math">\(z^{[l]} = w^{[l]} a^{[l-1]} + b^{[l]}\)</span></li>
<li><span class="math">\(a^{[l]} = g^{[l]}(z^{[l]})\)</span></li>
</ul>
<div class="figure">
<img src="../images/nn-3.png" />
</div></li>
<li><p>perform back-propagation: back propagate the error through each layer</p>
<ul>
<li><p>last layer</p>
<p><span class="math">\(dz^{[L]} = \frac{\partial \ J}{\partial \ z^{[l]}} = a^{[L]} - y\)</span></p>
<p><span class="math">\(dw^{[L]} = \frac{\partial \ J}{\partial \ w^{[L]}} = \frac{1}{m} dz^{[L]} a^{[L-1]}\)</span></p>
<p><span class="math">\(db^{[L]} = \frac{\partial \ J}{\partial b^{[L]}} = \frac{1}{m} dz^{[L]}\)</span></p></li>
<li><p>previous layers</p>
<p><span class="math">\(dz^{[l]} = w^{[l+1]^T} dz^{[l+1]} * g&#39;^{[l]}(z^{[l]})\)</span></p>
<p><span class="math">\(dw^{[l]} = \frac{1}{m} dz^{[l+1]} a^{[l-1]}\)</span></p>
<p><span class="math">\(db^{[l]} = \frac{1}{m} dz^{[l+1]}\)</span></p></li>
<li><p>update the weights and biases for every layer</p>
<p><span class="math">\(w^{[l]} = w^{[l]} - \alpha dw^{[l]}\)</span></p>
<p><span class="math">\(b^{[l]} = b^{[l]} - \alpha db^{[l]}\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</body>
</html>
