<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="neural-networks-for-classification-or-regression"><span class="header-section-number">1</span> Neural networks (for classification or regression)</h1>
<p>Used to estimate unknown functions (complex, non-linear hypothesis) that are based on a large number of inputs, through the back-propagation algorithm.<br />Generally more complex and computationally expensive than other methods, but powerful for certain problems.<br />The basis of many deep learning methods.<br />Today is the state of the art technique for many different machine learning problems.</p>
<h2 id="description"><span class="header-section-number">1.1</span> Description</h2>
<p>Neural networks were developed to vaguely simulate the neurons in the brain.</p>
<p>A neuron is a computational unit that receives a number of inputs through its input wires (<em>dendrites</em>), does some computation and then sends signals through its output wire (<em>axon</em>) to other neurons in the brain.<br />A neural network is a group of neurons. The inputs are groupes in an <em>input layer</em>. The outputs are grouped in a final <em>output layer</em>. The layers in between are called the <em>hidden layers</em>.<br />Adding more layers helps computing even more complex functions on the input data.</p>
<h2 id="notation"><span class="header-section-number">1.2</span> Notation</h2>
<ul>
<li><p>training set <span class="math">\(\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\}\)</span> with <span class="math">\(m\)</span> training samples</p></li>
<li><p>input variable with <span class="math">\(n\)</span> features: <span class="math">\(x^{(i)}=\{x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}\}\)</span></p></li>
<li><p>output variable <span class="math">\(y^{(i)}\)</span>, represented by either a single value (in case of regression or binary classification) or by a probability vector (in case of multi-class classification)</p></li>
<li><p><span class="math">\(L\)</span>: the total number of layers in the network (including the input and the output layer)</p></li>
<li><p><span class="math">\(s_l\)</span>: the number of units (neurons) in layer <span class="math">\(l\)</span></p></li>
<li><p><span class="math">\(\Theta^{(l)}\)</span>: matrix of weights (parameters) controlling function mapping from layer <span class="math">\(l-1\)</span> to layer <span class="math">\(l\)</span></p></li>
<li><p><span class="math">\(\Theta_{ij}^{(l)}\)</span>: weight from unit <span class="math">\(i\)</span> in layer <span class="math">\(l-1\)</span> to unit <span class="math">\(j\)</span> in layer <span class="math">\(l\)</span></p></li>
<li><p><span class="math">\(z_j^{(l)} = \Theta_{0j}^{(l)} a_0^{(l-1)} + \Theta_{1j}^{(l)} a_1^{(l-1)} + ... + \Theta_{s_{l-1}j}^{(l)} a_{s_{l-1}}^{(l-1)}\)</span>: product between weights and inputs</p></li>
<li><p><span class="math">\(a_j^{(l)} = g(z_j^{(l)})\)</span>: <em>activation</em> of unit <span class="math">\(j\)</span> in layer <span class="math">\(l\)</span></p></li>
<li><p>output <span class="math">\(\hat{y} = h_{\Theta}(x)\)</span></p></li>
</ul>
<h2 id="neural-networks-for-multi-class-classification"><span class="header-section-number">1.3</span> Neural networks for multi-class classification</h2>
<ul>
<li><p>Model's architecture</p>
<ul>
<li>fully connected network</li>
<li><span class="math">\(L\)</span> layers</li>
<li>input layer with <span class="math">\(m\)</span> units (a training sample <span class="math">\(x\)</span>)</li>
<li>output layer with <span class="math">\(K\)</span> units (<span class="math">\(\hat{y} = h_{\Theta}(x) \in \mathbb{R}^{K}\)</span>)</li>
<li><span class="math">\(L-2\)</span> hidden layers with <span class="math">\(s^{(l)}\)</span> units (for <span class="math">\(l \in \{2,...,L-1\}\)</span>)</li>
</ul></li>
<li><p>Model's parameters</p>
<p><span class="math">\(\Theta=\{\theta^{(1)}, \theta^{(2)}, ..., \theta^{(L)}\}\)</span></p></li>
<li><p>Cost function</p>
<p><span class="math">\(J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} \left( -y_k^{(i)}\ log\ \hat{y}_k^{(i)} - (1 - y_k^{(i)})\ log\ (1 - \hat{y}_k^{(i)}) \right)\)</span></p></li>
<li><p>Goal</p>
<p><span class="math">\(\underset{\Theta}{\operatorname{min}} J(\Theta)\)</span></p></li>
<li><p>Algorithm</p>
<ul>
<li><p>start with some initial values for <span class="math">\(\Theta\)</span> (usually random values in <span class="math">\([-\epsilon, \epsilon]\)</span>)</p></li>
<li><p>set <span class="math">\(\Delta_{ij}^{l} = 0\)</span> (accumulate the partial derivatives <span class="math">\(\frac{\partial}{\partial\Theta_{ij}^{(l)}} J(\Theta)\)</span>)</p></li>
<li><p>for <span class="math">\(i = 1\ to \ m\)</span></p>
<ul>
<li><p>set <span class="math">\(a^{(1)} = x^{(i)}\)</span></p></li>
<li><p>perform forward-propagation to compute <span class="math">\(a^{(l)}\)</span> for <span class="math">\(l=\{2, 3, ..., L\}\)</span></p>
<ul>
<li><span class="math">\(z^{(l)} = \Theta^{(l)} a^{(l-1)}\)</span></li>
<li><span class="math">\(a^{(l)} = g(z^{(l)})\)</span></li>
</ul></li>
<li><p>compute the output error <span class="math">\(\delta^{(L)} = a^{(L)} - y^{(i)}\)</span></p></li>
<li><p>perform back-propagation</p>
<ul>
<li><p>back propagate error trough each layer <span class="math">\(\delta^{(L-1)}, \delta^{(L-2)}, ..., \delta^{(2)}\)</span></p>
<p><span class="math">\(\delta^{(l-1)} = (\Theta^{l-1})^T \delta^{(l)} \cdot \left( a^{(l)} * (1 - a^{(l)})\right)\)</span></p></li>
<li><p>compute the gradients: <span class="math">\(\Delta_{ij}^{l} = \Delta_{ij}^{l} + a_j^{(l)} \delta_i^{(l+1)}\)</span></p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</body>
</html>
