<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<h1 id="naive-bayes-classifier"><span class="header-section-number">1</span> Naive Bayes Classifier</h1>
<p>Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong, naive <em>independence assumptions</em> between the features.</p>
<p>The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of features. They are simple, scalable classification algorithms used especially in text classification tasks.</p>
<p>It uses the concept of <em>probability</em> to clasify new instances.</p>
<h2 id="description"><span class="header-section-number">1.1</span> <a href="https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/">Description</a></h2>
<p>The Naive Bayes algorithm is an intuitive method that uses the probabilities of each attribute belonging to each class to make a prediction.</p>
<p>Naive bayes simplifies the calculation of probabilities by assuming that the probability of each attribute belonging to a given class value is independent of all other attributes.<br />This is a strong assumption but results in a fast and effective method.</p>
<p>The probability of a class value given a value of an attribute is called the conditional probability.<br />By multiplying the conditional probabilities together for each attribute for a given class value, we have a probability of a data instance belonging to that class.</p>
<p>To make a prediction we can calculate probabilities of the instance belonging to each class and select the class value with the highest probability.</p>
<h2 id="naive-bayes-probabilistic-model"><span class="header-section-number">1.2</span> Naive Bayes probabilistic model</h2>
<p>Naive Bayes is a <strong>conditional probability model</strong>.<br />It answers the question &quot;what is the probability that something will happen, given that something else has already happened?&quot;.<br />Given a problem instance to be classified, represented by a vector <span class="math">\(x=\{x_1, ..., x_n\}\)</span> with <span class="math">\(n\)</span> features (independent variables), it assigns to it class probabilities <span class="math">\(p(C_k \mid x_1, ..., x_n)\)</span>.</p>
<p>Using <strong>Bayes' theorem</strong> <span class="math">\(\ \ P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}\ \ \)</span> the conditional probability can be decomposed as <span class="math">\(\ \ p(C_k \mid x)=\frac{p(C_k) \ p(x \mid C_k)}{p(x)}\)</span>. In other words: <span class="math">\(\ \ \text{posterior}=\frac{ \text{prior} \ \cdot \ \text{likelihood}}{\text{evidence}}\)</span></p>
<div class="figure">
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172.png" />
</div>
<p>In practice the denominator is ignored, given it's constant.</p>
<p>The numerator is equivalent to the joint probability model <span class="math">\(p(C_k,x_1,...,x_n)\)</span>, which can be rewritten using the <strong>chain rule</strong></p>
<ul>
<li><span class="math">\(P(A_n, ..., A_3, A_2, A_1) = P(A_n \mid A_{n-1}, ..., A_3, A_2, A_1) ... P(A_3 \mid A_2, A_1) P(A_2 \mid A_1) P(A_1)\)</span></li>
</ul>
<p>as</p>
<ul>
<li><span class="math">\(p(C_k, x_1, ..., x_n) = p(x_1, ..., x_n, C_k) = p(x_1 \mid x_2, ..., x_n, C_k) p(x_2, ..., x_n, C_k) = ... = p(x_1 \mid x_2, ..., x_n, C_k) p(x_2 \mid x_3, ..., x_n, C_k) ... p(x_{n-1} \mid x_n, C_k) p(x_n \mid C_k) P(C_k)\)</span></li>
</ul>
<p>Now the &quot;naive&quot; conditional independence assumptions come into play: assume that each feature <span class="math">\(x_i\)</span> is conditionally independent of every other feature <span class="math">\(x_j\)</span> for <span class="math">\(j\neq i\)</span>, given the category <span class="math">\(C_k\)</span>.<br />Each piece of evidence is hence treated as independent.<br />This means that <span class="math">\(\ \ p(x_j \mid x_{j+1}, ..., x_n, C_k) = p(x_j \mid C_k)\)</span></p>
<p>Thus, the joint model can be expressed as</p>
<p><span class="math">\(p(C_k \mid x_1, ..., x_n) \varpropto p(C_k, x_1, ..., x_n) = p(C_k) \ p(x_1 \mid C_k) \ p(x_2 \mid C_k) \ p(x_3 \mid C_k) \ ... \ p(x_n \mid C_k) = p(C_k) \ \prod_{j=1}^{n} p(x_j \mid C_{k})\)</span></p>
<p>Notes</p>
<ul>
<li>probability model: <span class="math">\(\ \ P(C_k \mid x) = p(C_k) \prod_{j=1}^{n} p(x_j \mid C_{k})\)</span></li>
<li>in <a href="https://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification">plain English</a>: <span class="math">\(\ \ P(\text{outcome} \mid \text{multiple evidence}) = P(\text{evidence}_1 \mid \text{outcome}) P(\text{evidence}_2 \mid \text{outcome}) ... P(\text{evidence}_n \mid \text{outcome}) P(\text{outcome})\)</span></li>
<li>if <span class="math">\(P(\text{evidence}_j \mid \text{outcome})=0\)</span>, then the whole probability becomes 0; contradicting evidence rules out the outcome</li>
<li>the intuition behind multiplying by the <em>prior</em> is to give high probability to more common outcomes, and low probabilities to unlikely outcomes; these are also called <em>base rates</em> and they are a way to scale predicted probabilities</li>
</ul>
<h2 id="naive-bayes-classifier-1"><span class="header-section-number">1.3</span> Naive Bayes classifier</h2>
<p>The naive Bayes classifier combines the <strong>naive Bayes probabilistic model</strong> with a <strong>decision rule</strong>.</p>
<p>Possible decision rules</p>
<ul>
<li><p>maximum likelihood (ML)</p>
<p>Find the parameter values that maximize the likelihood function.</p>
<p><span class="math">\(\hat{y} = \underset{y}{\operatorname{argmax}} \prod_{j=1}^{n} p(x_j \mid y)\)</span></p></li>
<li><p>maximum a posteriori (MAP)</p>
<p>Pick the hypothesis that is <strong>most probable</strong>.</p>
<p><span class="math">\(\hat{y} = \underset{y}{\operatorname{argmax}} p(y) \prod_{j=1}^{n} p(x_j \mid y)\)</span></p></li>
<li><p>recalibrated likelihood</p>
<p>Replace the class distribution with a set of weights learned from the data.</p>
<p><span class="math">\(\hat{y} = \underset{y}{\operatorname{argmax}} w_y \prod_{j=1}^{n} p(x_j \mid y)\)</span></p></li>
</ul>
<p>The ML classification is equivalent to the MAP classification with a uniform class distribution.</p>
<h2 id="parameter-estimation"><span class="header-section-number">1.4</span> Parameter estimation</h2>
<p>A class's prior may be calculated by</p>
<ul>
<li>assuming equiprobable classes: <span class="math">\(p(c_k) = \frac{1}{K}\)</span></li>
<li>calculating an estimate for the class probability from the training set: <span class="math">\(p(c_k) = \frac{\text{# samples in the class } c_k}{\text{# samples}}\)</span></li>
</ul>
<p>To estimate the parameters for a feature's distribution <span class="math">\(p(x|c_k)\)</span>, one must assume a distribution or generate non-parametric models for the features from the training set.</p>
<p>The <strong>assumptions on distributions</strong> of features are called the event model of the Naive Bayes classifier.<br />For discrete features, multinomial and Bernoulli distributions are popular.<br />For continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution.</p>
<h3 id="gaussian-naive-bayes"><span class="header-section-number">1.4.1</span> Gaussian naive Bayes</h3>
<p>Implements the Gaussian Naive Bayes algorithm for classification.<br />The likelihood of the features is assumed to be Gaussian</p>
<p><span class="math">\(p(x_j \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_j - \mu_y)^2}{2\sigma^2_y}\right)\)</span></p>
<p>Estimate the parameters <span class="math">\(\sigma_y\)</span> and <span class="math">\(\mu_y\)</span> for each feature and for each class from the training set.</p>
<h3 id="multinomial-naive-bayes"><span class="header-section-number">1.4.2</span> Multinomial naive Bayes</h3>
<p>MultinomialNB implements the naive Bayes algorithm for multinomially distributed data.</p>
<p>The distribution is parametrized by vectors <span class="math">\(\theta_y = \{\theta_{y_1},\ldots,\theta_{y_n}\}\)</span> for each class <span class="math">\(y\)</span>, where <span class="math">\(\theta_{y_j} = p(x_j \mid y)\)</span> of feature <span class="math">\(j\)</span> appearing in a sample belonging to class <span class="math">\(y\)</span>.</p>
<p>The parameters <span class="math">\(\theta_y\)</span> are estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:</p>
<p><span class="math">\(\hat{\theta}_{y_j} = \frac{ N_{y_j} + \alpha}{N_y + \alpha n}\)</span></p>
<p>where <span class="math">\(N_{y_j} = \sum_{x \in T} x_j\)</span> is the number of times feature <span class="math">\(j\)</span> appears in a sample of class <span class="math">\(y\)</span> in the training set <span class="math">\(T\)</span>, and <span class="math">\(N_{y} = \sum_{j=1}^{|T|} N_{y_j}\)</span> is the total count of all features for class <span class="math">\(y\)</span>.</p>
<p>The smoothing priors <span class="math">\(\alpha \ge 0\)</span> accounts for features not present in the learning samples and prevents zero probabilities in further computations.<br />Setting <span class="math">\(\alpha = 1\)</span> is called <em>Laplace smoothing</em>, while <span class="math">\(\alpha &lt; 1\)</span> is called <em>Lidstone smoothing</em>.</p>
<h3 id="bernoulli-naive-bayes"><span class="header-section-number">1.4.3</span> Bernoulli naive Bayes</h3>
<p>BernoulliNB implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.</p>
<p>The decision rule for Bernoulli naive Bayes is based on</p>
<p><span class="math">\(P(x_j \mid y) = P(j \mid y) x_j + (1 - P(j \mid y)) (1 - x_j)\)</span></p>
<p>which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature <span class="math">\(i\)</span> that is an indicator for class <span class="math">\(y\)</span>, where the multinomial variant would simply ignore a non-occurring feature.</p>
<h2 id="algorithm-for-gaussiannb"><span class="header-section-number">1.5</span> Algorithm for GaussianNB</h2>
<ul>
<li><p>data description</p>
<ul>
<li><p><span class="math">\(m\)</span> input variables <span class="math">\(x^{(i)}\)</span> represented by <span class="math">\(n\)</span> features: <span class="math">\(x^{(i)} = \{x_1^{(i)}, x_2^{(i)}, ..., x_n^{(i)}\}\)</span></p></li>
<li><p>each input variable has a target variable <span class="math">\(y^{(i)}\)</span> wich has one of <span class="math">\(K\)</span> possible categories</p></li>
</ul></li>
<li><p>model description</p>
<ul>
<li><p>probabilistic model: <span class="math">\(\ \ P(y \mid x) = p(y) \prod_{j=1}^{n} p(x_j \mid y)\)</span></p></li>
<li><p>MAP hypothesis: <span class="math">\(\hat{y} = \underset{k \in \{1,...,K\}}{\operatorname{argmax}} p(y) \prod_{j=1}^{n} p(x_j \mid y)\)</span></p></li>
<li><p>Gaussian feature distribution <span class="math">\(P(x_j \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_j - \mu_y)^2}{2\sigma^2_y}\right)\)</span></p></li>
</ul></li>
<li><p>algorithm</p>
<ul>
<li><p>separate data by class</p></li>
<li><p>compute the class prior probability for each class</p></li>
<li><p>compute the mean and variance values of each feature and each class</p></li>
<li><p>make predictions on new examples</p>
<ul>
<li><p>compute the probability that the given instance belong to each class</p></li>
<li><p>select the class with the largest probability</p></li>
</ul></li>
</ul></li>
</ul>
<h2 id="pros-and-cons-of-naive-bayes"><span class="header-section-number">1.6</span> <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/">Pros and Cons of Naive Bayes</a></h2>
<p>Pros</p>
<ul>
<li>it is easy and fast to predict class of test data set</li>
<li>it perform well in multi class prediction</li>
<li>it works well with highly sparsed datasets</li>
<li>requires a small amount of training data to estimate the necessary parameters</li>
<li>when the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression and needs less training data</li>
<li>it perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).</li>
</ul>
<p>Cons</p>
<ul>
<li>if categorical variable has a category (in test data set), which was not observed in training data set, then model will assign it a 0 probability and will not be able to make a prediction. This is often known as <em>zero frequency</em>. Using a smoothing technique (e.g. Laplace) can solve this problem</li>
<li>naive Bayes is known as a bad estimator, so the probability outputs are not to be taken too seriously</li>
<li>in real life, it is almost impossible that we get a set of predictors which are completely independent</li>
</ul>
</body>
</html>
